{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Presentation :\n\nThe purpose of this notebook is to build a GAN to generate/discriminate e-commerce clothings images. The dataset used is the Fashion MNIST from tensorflow datasets.","metadata":{}},{"cell_type":"markdown","source":"## Install dependencies that might not be already installed in kaggle environnement","metadata":{}},{"cell_type":"code","source":"pip install matplotlib tensorflow-datasets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import libraries + hardware test","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\ngpus = tf.config.experimental.list_physical_devices('GPU')\n\nfor gpu in gpus:\n    \n    print(gpu)\n    tf.config.experimental.set_memory_growth(gpu, True)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T11:48:10.511732Z","iopub.execute_input":"2024-04-29T11:48:10.512044Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"2024-04-29 11:48:10.843683: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-29 11:48:10.843749: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-29 11:48:10.845227: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"strategy = tf.distribute.MirroredStrategy()\n\n# Print the number of devices detected\nprint('Number of devices: {}'.format(strategy.num_replicas_in_sync))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load dataset from tensorflow dataset","metadata":{}},{"cell_type":"code","source":"dataset = tfds.load('fashion_mnist', split='train')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualize small sample of the data","metadata":{}},{"cell_type":"code","source":"image_pipeline = dataset.as_numpy_iterator()\nimage_pipeline.next()['image']\n\nfig, ax = plt.subplots(ncols=4, figsize=(20,20))\n\nfor idx in range(4): \n    sample = image_pipeline.next()\n    ax[idx].imshow(np.squeeze(sample['image']))\n    ax[idx].title.set_text(sample['label'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"def scale_images(data): \n    image = data['image']\n    return image / 255\n\ndataset = tfds.load('fashion_mnist', split='train')\ndataset = dataset.map(scale_images) # Parallelizing Data Transformation\ndataset = dataset.cache()\ndataset = dataset.shuffle(10000)  # Adjusted buffer size for memory management\ndataset = dataset.batch(256)  # Adjust based on your GPU\ndataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)  # Auto-tune the prefetch size\ndataset = strategy.experimental_distribute_dataset(dataset) # Leverage the two GPU","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, Dense, Flatten, Reshape, LeakyReLU, Dropout, UpSampling2D, BatchNormalization","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build generator model","metadata":{}},{"cell_type":"code","source":"def build_generator():\n    model = Sequential()\n\n    # Input layer\n    model.add(Dense(7*7*128, input_dim=128))\n    model.add(LeakyReLU(0.2))\n    model.add(Reshape((7,7,128)))\n\n    # Upsampling block 1\n    model.add(UpSampling2D())\n    model.add(Conv2D(128, 5, padding='same'))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU(0.2))\n\n    # Upsampling block 2\n    model.add(UpSampling2D())\n    model.add(Conv2D(128, 5, padding='same'))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU(0.2))\n\n    # Convolutional block 1\n    model.add(Conv2D(128, 4, padding='same'))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU(0.2))\n\n    # Convolutional block 2\n    model.add(Conv2D(128, 4, padding='same'))\n    model.add(BatchNormalization())\n    model.add(LeakyReLU(0.2))\n\n    # Output conv layer\n    model.add(Conv2D(1, 4, padding='same', activation='tanh'))\n\n    return model\n\ngenerator = build_generator()\ngenerator.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test untrained generator","metadata":{}},{"cell_type":"code","source":"img = generator.predict(np.random.randn(4,128,1))\n# Generate new fashion\nimg = generator.predict(np.random.randn(4,128,1))\n# Setup the subplot formatting \nfig, ax = plt.subplots(ncols=4, figsize=(20,20))\n# Loop four times and get images \nfor idx, img in enumerate(img): \n    # Plot the image using a specific subplot \n    ax[idx].imshow(np.squeeze(img))\n    # Appending the image label as the plot title \n    ax[idx].title.set_text(idx)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build discriminator","metadata":{}},{"cell_type":"code","source":"def build_discriminator():\n    model = Sequential()\n    \n    # First Conv Block\n    model.add(Conv2D(32, 5, strides=(2, 2), padding='same', input_shape=(28, 28, 1)))\n    model.add(LeakyReLU(0.2))\n    \n    # Second Conv Block\n    model.add(Conv2D(64, 5, strides=(2, 2), padding='same'))\n    model.add(LeakyReLU(0.2))\n    \n    # Third Conv Block\n    model.add(Conv2D(128, 5, strides=(2, 2), padding='same'))\n    model.add(LeakyReLU(0.2))\n    \n    # Fourth Conv Block\n    model.add(Conv2D(256, 5, strides=(2, 2), padding='same'))\n    model.add(LeakyReLU(0.2))\n    \n    # Flatten then pass to dense layer\n    model.add(Flatten())\n    model.add(Dense(1, activation='sigmoid'))\n    \n    return model\n\ndiscriminator = build_discriminator()\ndiscriminator.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import BinaryCrossentropy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.metrics import Mean","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Build GAN architecture","metadata":{}},{"cell_type":"code","source":"class FashionGAN(Model): \n    def __init__(self, generator, discriminator, *args, **kwargs):\n        # Pass through args and kwargs to base class \n        super().__init__(*args, **kwargs)\n        \n        # Create attributes for gen and disc\n        self.generator = generator \n        self.discriminator = discriminator \n        \n    def compile(self, g_opt, d_opt, g_loss, d_loss, *args, **kwargs): \n        # Compile with base class\n        super().compile(*args, **kwargs)\n        \n        # Create attributes for losses and optimizers\n        self.g_opt = g_opt\n        self.d_opt = d_opt\n        self.g_loss = g_loss\n        self.d_loss = d_loss \n\n    def train_step(self, batch):\n        # Get the data \n        real_images = batch\n        fake_images = self.generator(tf.random.normal((128, 128, 1)), training=False)\n        \n        # Train the discriminator\n        with tf.GradientTape() as d_tape: \n            # Pass the real and fake images to the discriminator model\n            yhat_real = self.discriminator(real_images, training=True) \n            yhat_fake = self.discriminator(fake_images, training=True)\n            yhat_realfake = tf.concat([yhat_real, yhat_fake], axis=0)\n            \n            # Create labels for real and fakes images\n            y_realfake = tf.concat([tf.zeros_like(yhat_real), tf.ones_like(yhat_fake)], axis=0)\n            \n            # Add some noise to the TRUE outputs\n            noise_real = 0.15*tf.random.uniform(tf.shape(yhat_real))\n            noise_fake = -0.15*tf.random.uniform(tf.shape(yhat_fake))\n            y_realfake += tf.concat([noise_real, noise_fake], axis=0)\n            \n            # Calculate loss - BINARYCROSS \n            total_d_loss = self.d_loss(y_realfake, yhat_realfake)\n            \n        # Apply backpropagation - nn learn \n        dgrad = d_tape.gradient(total_d_loss, self.discriminator.trainable_variables) \n        self.d_opt.apply_gradients(zip(dgrad, self.discriminator.trainable_variables))\n        \n        # Train the generator \n        with tf.GradientTape() as g_tape: \n            # Generate some new images\n            gen_images = self.generator(tf.random.normal((128,128,1)), training=True)\n                                        \n            # Create the predicted labels\n            predicted_labels = self.discriminator(gen_images, training=False)\n                                        \n            # Calculate loss - trick to training to fake out the discriminator\n            total_g_loss = self.g_loss(tf.zeros_like(predicted_labels), predicted_labels) \n            \n        # Apply backprop\n        ggrad = g_tape.gradient(total_g_loss, self.generator.trainable_variables)\n        self.g_opt.apply_gradients(zip(ggrad, self.generator.trainable_variables))\n        \n        return {\"d_loss\":total_d_loss, \"g_loss\":total_g_loss}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Instantiation of the GAN + scope the training loop to use all GPUs","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    generator = build_generator()\n    discriminator = build_discriminator()\n\n    # Create instance of subclassed model\n    fashgan = FashionGAN(generator, discriminator)\n\n    # Optimizers and loss functions\n    g_opt = Adam(learning_rate=0.000002, beta_1=0.5)\n    d_opt = Adam(learning_rate=0.0000002, beta_1=0.5)\n\n    g_loss = BinaryCrossentropy()\n    d_loss = BinaryCrossentropy()\n\n    # Compile the GAN model\n    fashgan.compile(g_opt, d_opt, g_loss, d_loss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train the GAN","metadata":{}},{"cell_type":"code","source":"hist = fashgan.fit(dataset, epochs=5000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot the loss during training","metadata":{}},{"cell_type":"code","source":"plt.suptitle('Loss')\nplt.plot(hist.history['d_loss'], label='d_loss')\nplt.plot(hist.history['g_loss'], label='g_loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generate images with the trained generator","metadata":{}},{"cell_type":"code","source":"noise = tf.random.normal((16, 128))\nimgs = generator(noise, training=False)\nprint(\"Generated images shape:\", imgs.shape)\n\nif imgs.shape == (16, 28, 28, 1):  # Expected shape\n    fig, ax = plt.subplots(ncols=4, nrows=4, figsize=(10, 10))\n    ax = ax.flatten()\n    for i in range(16):\n        ax[i].imshow(imgs[i, :, :, 0])\n        ax[i].axis('off')\n    plt.show()\nelse:\n    print(\"Unexpected shape of generated images:\", imgs.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generator.save('generator.h5')\ndiscriminator.save('discriminator.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2nd attempt : Leveraging pre-trained model instead of training from scratch","metadata":{}},{"cell_type":"code","source":"import tensorflow_hub as hub\nfrom tensorflow.keras.layers import Dense, Flatten, Reshape, Input, Conv2DTranspose, Conv2D, BatchNormalization, LeakyReLU, UpSampling2D, Reshape\nfrom tensorflow.keras.applications import ResNet50\n\ndef build_generator():\n    model = tf.keras.Sequential([\n        tf.keras.layers.Input(shape=(100,)),\n        tf.keras.layers.Dense(7*7*256, use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.LeakyReLU(),\n        tf.keras.layers.Reshape((7, 7, 256)),\n        tf.keras.layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.LeakyReLU(),\n        tf.keras.layers.UpSampling2D(),\n        tf.keras.layers.Conv2DTranspose(64, (5, 5), strides=(1, 1), padding='same', use_bias=False),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.LeakyReLU(),\n        tf.keras.layers.UpSampling2D(),\n        tf.keras.layers.Conv2DTranspose(1, (5, 5), strides=(1, 1), padding='same', use_bias=False, activation='tanh')\n    ])\n    return model\n\ndef build_discriminator():\n    # Define the input shape and preprocess inputs\n    inputs = Input(shape=(28, 28, 1))\n    x = UpSampling2D(size=(8, 8))(inputs)  # Upsample to 224x224\n    x = Conv2D(3, (3, 3), padding='same', activation='relu')(x)  # Convert to 3 channels\n    \n    # Utilize ResNet50 as a feature extractor\n    resnet_model = ResNet50(include_top=False, input_shape=(224, 224, 3), pooling='avg')\n    resnet_model.trainable = False  # Freeze the model\n    x = resnet_model(x)\n\n    # Flatten the output and add a Dense layer for binary classification\n    x = Flatten()(x)\n    outputs = Dense(1, activation='sigmoid')(x)\n\n    # Create the model\n    model = Model(inputs, outputs)\n    return model\n\nclass FashionGAN(Model): \n    def __init__(self, generator, discriminator, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.generator = generator \n        self.discriminator = discriminator \n        \n    def compile(self, g_opt, d_opt, g_loss, d_loss, *args, **kwargs): \n        super().compile(*args, **kwargs)\n        self.g_opt = g_opt\n        self.d_opt = d_opt\n        self.g_loss = g_loss\n        self.d_loss = d_loss \n\n    def train_step(self, data):\n        # Unpack the data tuple (images and labels)\n        real_images, _ = data  # Assuming labels are not used\n\n        batch_size = tf.shape(real_images)[0]\n        random_latent_vectors = tf.random.normal(shape=(batch_size, 100))\n\n        with tf.GradientTape() as d_tape, tf.GradientTape() as g_tape:\n            fake_images = self.generator(random_latent_vectors, training=True)\n            real_output = self.discriminator(real_images, training=True)\n            fake_output = self.discriminator(fake_images, training=True)\n\n            d_loss_real = self.d_loss(tf.zeros_like(real_output), real_output)\n            d_loss_fake = self.d_loss(tf.ones_like(fake_output), fake_output)\n            total_d_loss = d_loss_real + d_loss_fake\n\n            total_g_loss = self.g_loss(tf.zeros_like(fake_output), fake_output)\n\n        d_grads = d_tape.gradient(total_d_loss, self.discriminator.trainable_variables)\n        g_grads = g_tape.gradient(total_g_loss, self.generator.trainable_variables)\n\n        self.d_opt.apply_gradients(zip(d_grads, self.discriminator.trainable_variables))\n        self.g_opt.apply_gradients(zip(g_grads, self.generator.trainable_variables))\n\n        # Return a dictionary mapping metric names to their current value tensors\n        return {\"d_loss\": total_d_loss, \"g_loss\": total_g_loss}\n\n# Assume using TensorFlow's strategy for distributed training\nstrategy = tf.distribute.MirroredStrategy()\nwith strategy.scope():\n    generator = build_generator()\n    discriminator = build_discriminator()\n\n    fashgan = FashionGAN(generator, discriminator)\n    g_opt = Adam(learning_rate=0.0002, beta_1=0.5)\n    d_opt = Adam(learning_rate=0.0002, beta_1=0.5)\n    g_loss = BinaryCrossentropy()\n    d_loss = BinaryCrossentropy()\n\n    fashgan.compile(g_opt, d_opt, g_loss, d_loss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"# Load Fashion-MNIST dataset\n(train_images, train_labels), _ = tf.keras.datasets.fashion_mnist.load_data()\n\n# Normalize the images to [0, 1]\ntrain_images = train_images.astype('float32') / 255.0\n\n# Expand the dimensions to (28, 28, 1)\ntrain_images = train_images[..., tf.newaxis]\n\n# Create a TensorFlow dataset\ndataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n\n# Shuffle and batch the dataset\ndataset = dataset.shuffle(buffer_size=1024).batch(32)  # Adjust batch size as needed\n\n# Use 'prefetch' to improve performance\ndataset = dataset.prefetch(tf.data.AUTOTUNE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training ","metadata":{}},{"cell_type":"code","source":"hist = fashgan.fit(dataset, epochs=12)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generate image with the trained generator","metadata":{}},{"cell_type":"code","source":"noise = tf.random.normal((16, 100))\nimgs = generator(noise, training=False)\nprint(\"Generated images shape:\", imgs.shape)\n\nif imgs.shape == (16, 28, 28, 1):  # Expected shape\n    fig, ax = plt.subplots(ncols=4, nrows=4, figsize=(10, 10))\n    ax = ax.flatten()\n    for i in range(16):\n        ax[i].imshow(imgs[i, :, :, 0])\n        ax[i].axis('off')\n    plt.show()\nelse:\n    print(\"Unexpected shape of generated images:\", imgs.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generator.save('generator3.h5')\ndiscriminator.save('discriminator3.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generator.save_weights('generator_weights.weights.h5')\ndiscriminator.save_weights('discriminator_weights.weights.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!jupyter nbconvert --to pdf /kaggle/input/hello-world-turn-this-into-a-pdf/__notebook__.ipynb --output /kaggle/working/output.pdf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion\n\nHardware/Computing power and training time are a major hurdle in training and deploying deep learning model !\n\nAreas of improvement\nGo back to initial architecture without using pre-trained model\nSetup a more powerful NN training lab\nGive more training time to the model\nExperiment with different architecture for generator and discriminator\n","metadata":{}}]}